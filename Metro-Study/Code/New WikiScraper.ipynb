{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597668000863",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import requests, re, time\n",
    "import pandas_datareader\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns \n",
    "\n",
    "import statsmodels.api as sm\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define our Connector\n",
    "\n",
    "import requests,os,time\n",
    "def ratelimit(dt):\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(dt) # sleep one second.\n",
    "\n",
    "class Connector():\n",
    "  def __init__(self,logfile,overwrite_log=False,connector_type='requests',session=False,path2selenium='',n_tries = 5,timeout=30,waiting_time=0.5):\n",
    "    \"\"\"This Class implements a method for reliable connection to the internet and monitoring. \n",
    "    It handles simple errors due to connection problems, and logs a range of information for basic quality assessments\n",
    "    \n",
    "    Keyword arguments:\n",
    "    logfile -- path to the logfile\n",
    "    overwrite_log -- bool, defining if logfile should be cleared (rarely the case). \n",
    "    connector_type -- use the 'requests' module or the 'selenium'. Will have different since the selenium webdriver does not have a similar response object when using the get method, and monitoring the behavior cannot be automated in the same way.\n",
    "    session -- requests.session object. For defining custom headers and proxies.\n",
    "    path2selenium -- str, sets the path to the geckodriver needed when using selenium.\n",
    "    n_tries -- int, defines the number of retries the *get* method will try to avoid random connection errors.\n",
    "    timeout -- int, seconds the get request will wait for the server to respond, again to avoid connection errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialization function defining parameters. \n",
    "    self.n_tries = n_tries # For avoiding triviel error e.g. connection errors, this defines how many times it will retry.\n",
    "    self.timeout = timeout # Defining the maximum time to wait for a server to response.\n",
    "    self.waiting_time = waiting_time # define simple rate_limit parameter.\n",
    "    ## not implemented here, if you use selenium.\n",
    "    if connector_type=='selenium':\n",
    "      assert path2selenium!='', \"You need to specify the path to you geckodriver if you want to use Selenium\"\n",
    "      from selenium import webdriver \n",
    "      ## HIN download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "      assert os.path.isfile(path2selenium),'You need to insert a valid path2selenium the path to your geckodriver. You can download the latest geckodriver here: https://github.com/mozilla/geckodriver/releases'\n",
    "      self.browser = webdriver.Firefox(executable_path=path2selenium) # start the browser with a path to the geckodriver.\n",
    "\n",
    "    self.connector_type = connector_type # set the connector_type\n",
    "    \n",
    "    if session: # set the custom session\n",
    "      self.session = session\n",
    "    else:\n",
    "      self.session = requests.session()\n",
    "    self.logfilename = logfile # set the logfile path\n",
    "    ## define header for the logfile\n",
    "    header = ['id','project','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "    if os.path.isfile(logfile):        \n",
    "      if overwrite_log==True:\n",
    "        self.log = open(logfile,'w')\n",
    "        self.log.write(';'.join(header))\n",
    "      else:\n",
    "        self.log = open(logfile,'a')\n",
    "    else:\n",
    "      self.log = open(logfile,'w')\n",
    "      self.log.write(';'.join(header))\n",
    "    ## load log \n",
    "    with open(logfile,'r') as f: # open file\n",
    "        \n",
    "      l = f.read().split('\\n') # read and split file by newlines.\n",
    "      ## set id\n",
    "      if len(l)<=1:\n",
    "        self.id = 0\n",
    "      else:\n",
    "        self.id = int(l[-1][0])+1\n",
    "            \n",
    "  def get(self,url,project_name):\n",
    "    \"\"\"Method for connector reliably to the internet, with multiple tries and simple error handling, as well as default logging function.\n",
    "    Input url and the project name for the log (i.e. is it part of mapping the domain, or is it the part of the final stage in the data collection).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    url -- str, url\n",
    "    project_name -- str, Name used for analyzing the log. Use case could be the 'Mapping of domain','Meta_data_collection','main data collection'. \n",
    "    \"\"\"\n",
    "     \n",
    "    project_name = project_name.replace(';','-') # make sure the default csv seperator is not in the project_name.\n",
    "    if self.connector_type=='requests': # Determine connector method.\n",
    "      for _ in range(self.n_tries): # for loop defining number of retries with the requests method.\n",
    "        ratelimit(self.waiting_time)\n",
    "        t = time.time()\n",
    "        try: # error handling \n",
    "          response = self.session.get(url,timeout = self.timeout) # make get call\n",
    "\n",
    "          err = '' # define python error variable as empty assumming success.\n",
    "          success = True # define success variable\n",
    "          redirect_url = response.url # log current url, after potential redirects \n",
    "          dt = t - time.time() # define delta-time waiting for the server and downloading content.\n",
    "          size = len(response.text) # define variable for size of html content of the response.\n",
    "          response_code = response.status_code # log status code.\n",
    "          ## log...\n",
    "          call_id = self.id # get current unique identifier for the call\n",
    "          self.id+=1 # increment call id\n",
    "          #['id','project_name','connector_type','t', 'delta_t', 'url', 'redirect_url','response_size', 'response_code','success','error']\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row to be written in the log.\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write log.\n",
    "          self.log.flush()\n",
    "          return response,call_id # return response and unique identifier.\n",
    "\n",
    "        except Exception as e: # define error condition\n",
    "          err = str(e) # python error\n",
    "          response_code = '' # blank response code \n",
    "          success = False # call success = False\n",
    "          size = 0 # content is empty.\n",
    "          redirect_url = '' # redirect url empty \n",
    "          dt = t - time.time() # define delta t\n",
    "\n",
    "          ## log...\n",
    "          call_id = self.id # define unique identifier\n",
    "          self.id+=1 # increment call_id\n",
    "\n",
    "          row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row\n",
    "          self.log.write('\\n'+';'.join(map(str,row))) # write row to log.\n",
    "          self.log.flush()\n",
    "    else:\n",
    "      t = time.time()\n",
    "      ratelimit(self.waiting_time)\n",
    "      self.browser.get(url) # use selenium get method\n",
    "      ## log\n",
    "      call_id = self.id # define unique identifier for the call. \n",
    "      self.id+=1 # increment the call_id\n",
    "      err = '' # blank error message\n",
    "      success = '' # success blank\n",
    "      redirect_url = self.browser.current_url # redirect url.\n",
    "      dt = t - time.time() # get time for get method ... NOTE: not necessarily the complete load time.\n",
    "      size = len(self.browser.page_source) # get size of content ... NOTE: not necessarily correct, since selenium works in the background, and could still be loading.\n",
    "      response_code = '' # empty response code.\n",
    "      row = [call_id,project_name,self.connector_type,t,dt,url,redirect_url,size,response_code,success,err] # define row \n",
    "      self.log.write('\\n'+';'.join(map(str,row))) # write row to log file.\n",
    "      self.log.flush()\n",
    "    # Using selenium it will not return a response object, instead you should call the browser object of the connector.\n",
    "    ## connector.browser.page_source will give you the html.\n",
    "      return None,call_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = Connector('MetroLog.csv',overwrite_log=True,path2selenium='/Users/holger/Documents/Python/Harmsen_Repo/Metro-Study/Gecko/geckodriver',connector_type='selenium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping Wikipedia:\n",
    "def WikiScraper(no_metros):\n",
    "    url = 'https://da.wikipedia.org/wiki/Stationer_p%C3%A5_K%C3%B8benhavns_Metro'\n",
    "    response,call_id = connector.get(url,'Metroer')\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Creating DataFrame and dictionary for information-storing:\n",
    "    data = pd.DataFrame()\n",
    "    dic_list = []\n",
    "\n",
    "\n",
    "    for i in range(1,no_metros+1):\n",
    "        link = connector.browser.find_element_by_xpath(f'/html/body/div[3]/div[3]/div[5]/div/table[1]/tbody/tr[{i}]/td[1]/a')\n",
    "        link.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # READ INFORMATION\n",
    "        # 1.1) Appending all information to DataFrame\n",
    "        table = pd.read_html(connector.browser.page_source)[0]\n",
    "        data[f'Traits_{i}'] = table.iloc[:,0]\n",
    "        data[f'{i}'] = table.iloc[:,1]\n",
    "\n",
    "        # 1.2) Finding extra information, that the read_html doesn't pick up:\n",
    "        soup = BeautifulSoup(connector.browser.page_source, 'lxml')\n",
    "        name = soup.find('h1').text\n",
    "        data[f'Traits_{i}'][0] = 'Name' # Adding the name to first row of the df (This is always NaN, so doesn't delete vital data)\n",
    "        data[f'{i}'][0] = name\n",
    "\n",
    "        # 2) Creating a dictionary with all information, ensuring homogeneity between different WikiPages:\n",
    "        dic = {}\n",
    "        for j in range(0,len(data[f'Traits_{i}'])):\n",
    "            dic[str(data[f'Traits_{i}'][j])] = str(data[f'{i}'][j])\n",
    "\n",
    "        dic_list.append(dic)\n",
    "\n",
    "        # RETURN \n",
    "        connector.browser.get(url)\n",
    "        time.sleep(1)\n",
    "\n",
    "    return [data,dic_list]\n",
    "    #return dic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetroScrape = WikiScraper(44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexlist = ['Name','Adresse','Jernbane','Åbningsdato']\n",
    "MetroList = pd.DataFrame(index=indexlist)\n",
    "\n",
    "for i in range(0,44):\n",
    "    specs = []\n",
    "    for j in indexlist:\n",
    "        try: \n",
    "            specs.append(MetroScrape[1][i][j]) # Appending the relevant Metro-information (j-key from scrape-dict)\n",
    "        except:\n",
    "            specs.append('NaN')\n",
    "    # Indexing column names:\n",
    "    MetroList[i] = specs\n",
    "MetroList = MetroList.transpose()\n",
    "MetroList.columns = ['Name', 'Address', 'Railroad', 'Opening Date'] # Changing names to english\n",
    "\n",
    "\n",
    "# Cleaning for '[1]' (from Wikipedia-references):\n",
    "MetroList['Address'] = [i.replace('[1]','') for i in MetroList['Address']]\n",
    "MetroList['Address'] = [i.replace('[2]','') for i in MetroList['Address']]\n",
    "MetroList['Railroad'] = [i.replace('[2]','') for i in MetroList['Railroad']]\n",
    "MetroList['Opening Date'] = [i.replace('[1]','') for i in MetroList['Opening Date']]\n",
    "\n",
    "# Cleaning NaN-values:\n",
    "MetroList = MetroList.drop(MetroList[\"Address\"].loc[MetroList[\"Address\"]=='NaN'].index)\n",
    "MetroList.reset_index(inplace=True)\n",
    "del MetroList['index']\n",
    "\n",
    "# Creating dummy-variable for Cityring:\n",
    "MetroList['Cityring'] = 0\n",
    "for i in range(0,len(MetroList)):\n",
    "    if 'Cityringen' in MetroList['Railroad'][i]:\n",
    "        MetroList['Cityring'][i] = 1\n",
    "\n",
    "# Simplyfying Opening Date-column to year for use in regression analysis later:\n",
    "for i in range(0,len(MetroList)):\n",
    "    try:\n",
    "        year = int(MetroList['Opening Date'][i][-4:])\n",
    "        if 0 < year < 2021:\n",
    "            MetroList['Opening Date'][i] = int(MetroList['Opening Date'][i][-4:])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Cleaning the rest by hand, example: '13. december 1986 (S-tog), 12. oktober 2003 (Metro)' is cleaned to 2003\n",
    "for i in [7,32]:\n",
    "    MetroList['Opening Date'][i] = 2003\n",
    "MetroList['Opening Date'][25] = 2002\n",
    "for i in [23,27,31,34]:\n",
    "    MetroList['Opening Date'][i] = 2019\n",
    "MetroList = MetroList.drop(22)\n",
    "MetroList.reset_index(inplace=True)\n",
    "del MetroList['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                         Name                           Address  \\\n0  Aksel Møllers Have Station  Godthåbsvej 312000 Frederiksberg   \n1       Amager Strand Station  Italiensvej 72A2300 København S    \n2           Amagerbro Station  Amagerbro Torv 12300 København S   \n\n         Railroad Opening Date  Cityring  \n0      Cityringen         2019         1  \n1  Østamagerbanen         2007         0  \n2   Ørestadsbanen         2002         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Address</th>\n      <th>Railroad</th>\n      <th>Opening Date</th>\n      <th>Cityring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Aksel Møllers Have Station</td>\n      <td>Godthåbsvej 312000 Frederiksberg</td>\n      <td>Cityringen</td>\n      <td>2019</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Amager Strand Station</td>\n      <td>Italiensvej 72A2300 København S</td>\n      <td>Østamagerbanen</td>\n      <td>2007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Amagerbro Station</td>\n      <td>Amagerbro Torv 12300 København S</td>\n      <td>Ørestadsbanen</td>\n      <td>2002</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "MetroList.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing that the coordinates returned equals (39.78373,-100.445882) if there's a mistake (mostly, that the door and zip code are not separated by space) in the Address, I fix the very last issues manually in Excel in about a minute. This is no problem, as I am only working with 44 entries, and it would have been relatively difficult using RegEx, as the apartment numbers do not have the same number of digits and sometimes have letters in them. Too heterogenous. Speed before unneeded and superfluously complex code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to Excel:\n",
    "MetroList.to_excel('/Users/holger/Documents/Python/Harmsen_Repo/Metro-Study/Pickles/Metro Adresses for Final Touches.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling edited version:\n",
    "MetroList = pd.read_excel('/Users/holger/Documents/Python/Harmsen_Repo/Metro-Study/Pickles/Metro Adresses for Final Touches_DONE.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                         Name                            Address  \\\n0  Aksel Møllers Have Station  Godthåbsvej 31 2000 Frederiksberg   \n1       Amager Strand Station  Italiensvej 72A 2300 København S    \n2           Amagerbro Station  Amagerbro Torv 1 2300 København S   \n\n         Railroad  Opening Date  Cityring  \n0      Cityringen          2019         1  \n1  Østamagerbanen          2007         0  \n2   Ørestadsbanen          2002         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Address</th>\n      <th>Railroad</th>\n      <th>Opening Date</th>\n      <th>Cityring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>Aksel Møllers Have Station</td>\n      <td>Godthåbsvej 31 2000 Frederiksberg</td>\n      <td>Cityringen</td>\n      <td>2019</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>Amager Strand Station</td>\n      <td>Italiensvej 72A 2300 København S</td>\n      <td>Østamagerbanen</td>\n      <td>2007</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>Amagerbro Station</td>\n      <td>Amagerbro Torv 1 2300 København S</td>\n      <td>Ørestadsbanen</td>\n      <td>2002</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "del MetroList['Unnamed: 0']\n",
    "MetroList.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}